start with a clear domain model, separate business logic from infrastructure concerns, design around use-cases not technical layers, ensure each module has a single focused responsibility, define stable interfaces for all external dependencies, inject dependencies instead of hard-coding them, avoid tight coupling between modules, program to abstractions not concrete implementations, define contracts via interfaces or protocols, design extension points without modifying stable core logic, encapsulate complexity behind simple boundaries, prefer composition over inheritance, structure the system into domain, application, infrastructure, and presentation layers, externalize configuration away from code, avoid mixing business rules with IO operations, keep logic pure when feasible, separate read and write paths when beneficial, use lightweight data structures for pure data transport, implement adapters for external systems, ensure every component has exactly one reason to change, build minimal and stable public APIs, eliminate god-objects and oversized manager classes, prevent leaking of internal details across boundaries, keep all dependencies explicit and avoid global state, design everything for testability from the start, unit-test domain behavior thoroughly, log important events at system edges, treat errors as a part of the public interface, ensure components can be replaced or evolved independently, achieve performance through solid architecture not hacks, document architectural decisions concisely with ADRs, maintain strict separation of concerns and clear boundaries between all layers

  начинаем с формулирования доменной модели, отделяем бизнес-логику от инфраструктурных деталей, проектируем систему вокруг пользовательских сценариев а не технических слоёв, даём каждому модулю одну чёткую ответственность, определяем стабильные интерфейсы для всех внешних зависимостей, внедряем зависимости вместо жёсткого связывания, избегаем плотной связности модулей, программируем против абстракций а не конкретных реализаций, задаём контракты через интерфейсы или протоколы, проектируем точки расширения без изменения устойчивого ядра, инкапсулируем сложность за простыми и понятными границами, предпочитаем композицию наследованию, разделяем систему на слои: домен, приложение, инфраструктура, представление, выносим конфигурацию за пределы кода, избегаем смешивания бизнес-правил и операций ввода-вывода, сохраняем логику максимально чистой там где возможно, разделяем операции чтения и записи когда это целесообразно, используем лёгкие структуры данных для переноса информации, создаём адаптеры для внешних систем, гарантируем что каждый компонент имеет одну единственную причину для изменения, проектируем публичные API минимальными и стабильными, избегаем god-objects и гигантских менеджеров, предотвращаем утечку деталей реализации за пределы модулей, делаем зависимости явными и исключаем глобальные состояния, изначально проектируем код тестируемым, покрываем доменное поведение модульными тестами, логируем важные события на границах системы, рассматриваем ошибки как часть публичного контракта, обеспечиваем возможность замены или эволюции любого компонента независимо от остальных, достигаем производительности архитектурой а не хаотичными оптимизациями, документируем архитектурные решения через краткие ADR, поддерживаем строгие границы и чистое разделение ответственности между всеми слоями


====== gpt 5.1 extended thinking 

начинаем с понимания домена и формулирования бизнес-целей, собираем и фиксируем функциональные и нефункциональные требования, формируем общее словарное ядро и единый язык терминов с бизнесом, явно описываем границы системы и контексты взаимодействия, проектируем архитектуру от домена и сценариев использования а не от фреймворков, разделяем бизнес-логику и инфраструктуру на уровне идей и на уровне кода, строим систему вокруг use-case / application слоёв, избегаем прямой зависимости домена от баз данных, сетей и UI, сначала определяем модель домена, сущности и их инварианты, выделяем сущности, value-объекты и их ответственность, определяем агрегаты и их границы, проектируем доменные события и реакции на них, определяем интерфейсы репозиториев и сервисов домена, применяем паттерн порты и адаптеры (hexagonal) для изоляции инфраструктуры, каждая часть системы имеет одну чёткую ответственность, строго следуем SRP — один модуль, одна причина для изменения, следуем OCP — расширяем поведение через новые реализации вместо изменения стабильного кода, следуем LSP — подтипы не ломают ожидания клиентов, следуем ISP — дробим громоздкие интерфейсы на небольшие целевые, следуем DIP — высокоуровневый код зависит от абстракций а не от деталей, явно объявляем интерфейсы для всех внешних зависимостей, внедряем зависимости через конструкторы, фабрики или DI-контейнер, избегаем сервис-локаторов и скрытых глобальных синглтонов, проектируем модули с минимальными и понятными API, делаем связи между модулями направленными и как можно более слабыми, предпочитаем композицию наследованию при расширении поведения, наследование используем только для истинных иерархий и полиморфизма, избегаем god-objects и «менеджеров всего на свете», не позволяем утекать деталям реализации через публичные интерфейсы, держим модули маленькими и фокусными, декомпозируем сложные классы на более простые, явно разделяем слои: домен, приложение, инфраструктура, представление, не допускаем зависимостей из внутренних слоёв к внешним, UI зависит от application-слоя а не от домена напрямую, инфраструктура реализует порты домена а не диктует ему форму, выносим всю конфигурацию (URL, ключи, флаги) за пределы кода, применяем конфигурацию через окружение, конфиг-файлы и секрет-хранилища, не хардкодим чувствительные данные в репозитории, не смешиваем бизнес-логику с IO-операциями, работа с сетью, файлами и БД сконцентрирована в адаптерах и инфраструктуре, держим как можно больше функций чистыми (без побочных эффектов), разделяем операции чтения и записи там где это даёт пользу (CQRS-подход), для чтения используем простые оптимизированные модели, для записи — строгие доменные модели, используем простые структуры данных / DTO для переноса данных между слоями, не тащим доменные сущности в инфраструктурные и транспортные слои, проектируем контракты API стабильными и обратно совместимыми, версионируем внешние API и события, чётко описываем схемы сообщений и форматы данных, явно обрабатываем ошибки и исключения, проектируем ошибки и коды ответов как часть контракта, различаем бизнес-ошибки и технические сбои, не глушим исключения без логирования и реакции, валидацию данных проводим как на границах системы так и внутри домена, инварианты домена защищаются самим доменом а не только UI, проектируем безопасность с самого начала а не «потом», учитываем аутентификацию, авторизацию, аудит и приватность данных, минимизируем поверхность атаки и не раскрываем лишние детали, логируем ключевые события на границах системы и в критичных точках домена, используем структурированные логи для дальнейшего анализа, добавляем метрики и трассировку для наблюдаемости, проектируем систему так чтобы её можно было мониторить и алертить, выбираем протоколы и форматы взаимодействия исходя из требований а не моды, проектируем систему с учётом ошибок сети, задержек и временной недоступности сервисов, применяем ретраи, таймауты, circuit breaker там где это нужно, думаем о идемпотентности операций при повторных запросах, обеспечиваем корректную миграцию данных и эволюцию схем, закладываем стратегию миграций БД и обратной совместимости, используем feature-флаги для безопасного включения новых возможностей, проектируем код так чтобы можно было катить по частям и откатывать изменения, думаем о масштабировании по оси чтения и по оси записи, сначала проектируем простую масштабируемую архитектуру монолита, разделяем систему на сервисы только при реальной необходимости, при микросервисах чётко определяем границы контекстов и владение данными, минимизируем синхронные межсервисные зависимости, используем асинхронное взаимодействие и события когда это оправдано, избегаем распределённых транзакций и компенсируемых операций без необходимости, проектируем соглашения об idempotency и согласованности данных, заранее думаем о миграции схем сообщений и контрактов, пишем код так чтобы его было легко тестировать, отделяем бизнес-логику от фреймворков, покрываем домен юнит-тестами, проверяем интеграцию на уровне адаптеров и границ, используем тестовые двойники для внешних зависимостей, автоматизируем ключевые сценарии end-to-end тестами, поддерживаем быстрый и надёжный тестовый прогон, интегрируем тесты в CI/CD конвейер, проектируем структуру каталогов и модулей отражая архитектуру а не случайно, называем пакеты и пространства имён по ролям и слоям, используем читаемые и говорящие имена для классов, методов и переменных, избегаем абстрактных названий вроде Manager, Helper, Utils без контекста, документируем публичные контракты и ключевую бизнес-логику, избегаем избыточных комментариев к очевидному коду, вместо комментариев улучшаем структуру и названия, применяем код-ревью как обязательный этап, обсуждаем архитектурные решения и trade-off на уровне команды, фиксируем архитектурные решения в коротких ADR, регулярно пересматриваем архитектуру с учётом новых требований но не переписываем всё без необходимости, отделяем временные решения и долгосрочные, помечаем технический долг и планируем его погашение, следуем единым код-стайлам и форматированию, автоматизируем линтеры и форматеры, не оптимизируем преждевременно, сначала делаем код понятным и корректным, оптимизацию проводим только после измерений, используем профилирование для поиска узких мест, отказываемся от «магии» и слишком умных решений в пользу простоты и явности, сознательно ограничиваем стек технологий и фреймворков, минимизируем количество точек отказа, проектируем систему так чтобы отдельные компоненты можно было разворачивать, обновлять и заменять независимо, обеспечиваем обратную совместимость при эволюции протоколов и моделей, делаем систему устойчивой к частичным отказам, проектируем graceful degradation и fallback-сценарии, думаем о эксплуатационной стороне: логирование, мониторинг, алерты, дашборды, автоматизируем сборку, тесты, деплой и миграции, проектируем архитектуру так чтобы новому разработчику было легко войти, поддерживаем документацию в актуальном состоянии, упрощаем архитектуру когда можно не усложнять, постоянно балансируем между идеальной чистотой и прагматичностью, и везде где возможно поддерживаем строгие границы, явные контракты и чёткое разделение ответственности между слоями, модулями и командами

start with understanding the domain and defining business goals, gather and document functional and non-functional requirements, build a shared glossary and ubiquitous language with the business, explicitly describe system boundaries and interaction contexts, design the architecture from the domain and use cases rather than from frameworks, separate business logic and infrastructure at the conceptual level and in code, build the system around use-case / application layers, avoid direct dependencies from the domain to databases, networks, and UI, first define the domain model, entities, and their invariants, distinguish entities, value objects, and their responsibilities, define aggregates and their boundaries, design domain events and their reactions, define interfaces of repositories and domain services, apply the ports-and-adapters (hexagonal) pattern to isolate infrastructure, ensure each part of the system has one clear responsibility, strictly follow SRP — one module, one reason to change, follow OCP — extend behavior via new implementations instead of modifying stable code, follow LSP — subtypes do not break client expectations, follow ISP — split fat interfaces into small, focused ones, follow DIP — high-level code depends on abstractions, not details, explicitly declare interfaces for all external dependencies, inject dependencies via constructors, factories, or a DI container, avoid service locators and hidden global singletons, design modules with minimal and understandable APIs, keep dependencies between modules one-directional and as weak as possible, prefer composition over inheritance when extending behavior, use inheritance only for true hierarchies and polymorphism, avoid god objects and “manager of everything” classes, do not let implementation details leak through public interfaces, keep modules small and focused, decompose complex classes into simpler ones, explicitly separate layers: domain, application, infrastructure, presentation, do not allow dependencies from inner layers to outer layers, let the UI depend on the application layer rather than the domain directly, let infrastructure implement domain ports instead of dictating domain shape, move all configuration (URLs, keys, flags) out of code, apply configuration via environment, config files, and secret stores, never hardcode sensitive data in the repository, do not mix business logic with IO operations, keep network, file, and database access concentrated in adapters and infrastructure, keep as many functions as possible pure (without side effects), separate read and write operations where it brings value (CQRS-style), use simple optimized models for reads, use strict domain models for writes, use simple data structures / DTOs to transfer data between layers, do not drag domain entities into infrastructure and transport layers, design API contracts to be stable and backward compatible, version external APIs and events, describe message schemas and data formats clearly, handle errors and exceptions explicitly, design errors and response codes as part of the contract, distinguish business errors from technical failures, never swallow exceptions without logging and appropriate handling, validate data both at system boundaries and inside the domain, let domain invariants be enforced by the domain itself, not only by the UI, design security from the beginning, not “later”, account for authentication, authorization, audit, and data privacy, minimize the attack surface and avoid exposing unnecessary details, log key events at system boundaries and in critical domain points, use structured logging for further analysis, add metrics and tracing for observability, design the system so it can be monitored and alerted, choose communication protocols and formats based on requirements, not fashion, design the system with network errors, latency, and temporary service unavailability in mind, apply retries, timeouts, and circuit breakers where appropriate, think about idempotency of operations under retries, ensure correct data migration and schema evolution, plan a database migration and backward-compatibility strategy, use feature flags to enable new capabilities safely, design code so it can be rolled out incrementally and rolled back safely, think about scaling along the read axis and the write axis, start with a simple, scalable monolith architecture, split the system into services only when there is real need, when using microservices, clearly define bounded contexts and data ownership, minimize synchronous inter-service dependencies, use asynchronous communication and events when justified, avoid distributed transactions and compensating operations unless necessary, design conventions for idempotency and data consistency, think ahead about schema evolution for messages and contracts, write code so that it is easy to test, separate business logic from frameworks, cover the domain with unit tests, test integration at the level of adapters and boundaries, use test doubles for external dependencies, automate key end-to-end scenarios, keep the test suite fast and reliable, integrate tests into the CI/CD pipeline, design folder and module structure to reflect the architecture, not accidents, name packages and namespaces according to roles and layers, use readable, meaningful names for classes, methods, and variables, avoid abstract names like Manager, Helper, Utils without context, document public contracts and key business logic, avoid excessive comments on obvious code, improve structure and naming instead of relying on comments, treat code review as a mandatory step, discuss architectural decisions and trade-offs at the team level, record architectural decisions in short ADRs, regularly revisit the architecture in light of new requirements but avoid full rewrites without strong reasons, distinguish temporary solutions from long-term ones, mark technical debt and plan how to pay it off, follow a consistent code style and formatting, automate linters and formatters, do not optimize prematurely, first make the code clear and correct, optimize only after measurement, use profiling to find bottlenecks, avoid “magic” and overly clever solutions in favor of simplicity and explicitness, deliberately limit the technology and framework stack, minimize the number of single points of failure, design the system so that individual components can be deployed, updated, and replaced independently, ensure backward compatibility when evolving protocols and models, make the system resilient to partial failures, design graceful degradation and fallback scenarios, think about operability: logging, monitoring, alerts, dashboards, automate build, tests, deployment, and migrations, design the architecture so that new developers can onboard easily, keep documentation up to date, simplify the architecture whenever you can instead of complicating it, continuously balance between ideal cleanliness and pragmatism, and wherever possible maintain strict boundaries, explicit contracts, and clear separation of responsibilities between layers, modules, and teams



# microservices
start from business capabilities and bounded contexts, define services around domain boundaries not technical layers, keep each microservice small, cohesive, and with a single clear responsibility, give each service its own data store and schema, avoid sharing databases across services, communicate between services via well-defined APIs not shared tables, favor explicit contracts over implicit coupling, design services to be independently deployable and independently scalable, optimize for autonomy of teams owning services, treat each service as a separately versioned product, keep the domain logic inside the service and infrastructure at the edges, avoid chatty fine-grained RPC between services, prefer coarse-grained interactions, use synchronous calls only where necessary and latency-tolerant, favor asynchronous messaging and events for integration and decoupling, use an API gateway or edge layer to shield clients from internal topology, avoid letting external consumers call internal services directly, design idempotent operations for safe retries, model eventual consistency explicitly in workflows and UX, avoid distributed transactions where possible, use sagas and compensating actions for cross-service workflows, make failure a first-class case when designing flows, apply timeouts, retries with backoff, and circuit breakers on service calls, limit the fan-out of synchronous calls per request, design bulkheads to isolate failures between components, ensure each service can start, stop, and restart without human intervention, keep service startup order loosely coupled, externalize configuration and secrets for each service, use environment-based configuration and centralized secret management, standardize logging, metrics, and tracing across all services, include correlation IDs in logs and traces for request flows, make every service observable: logs, metrics, health checks, readiness and liveness probes, define clear SLOs and error budgets per service, expose lightweight health endpoints for orchestration and load balancers, automate deployment, rollback, and scaling for each service, use containers as a default packaging mechanism, rely on orchestration (like Kubernetes-style) for scheduling and resilience, build immutable artifacts and promote them across environments, keep CI/CD pipelines per service and keep them fast, test services in isolation with contract tests, use consumer-driven contracts to protect integrations, minimize reliance on large, brittle end-to-end tests, keep APIs backward compatible and version them when breaking changes are needed, treat API schemas and message formats as code under version control, deprecate old API versions with clear timelines, document APIs clearly and keep docs close to code, prefer stable interfaces over clever ones, avoid leaking internal implementation details through public APIs, avoid central “god services” that everything depends on, design for decentralized governance with a small set of global standards, limit the number of technologies and runtimes to reduce cognitive load, but allow justified exceptions, keep shared libraries minimal and focused to avoid tight coupling, prefer sharing concepts via documentation and APIs not shared code, think about multi-tenancy and data isolation from day one, encrypt data in transit and at rest, design authorization and authentication at the service boundary, use a consistent identity and access model across services, avoid duplicating security logic in ad hoc ways, consider a zero-trust approach between services, validate inputs at the edge of each service, enforce domain invariants inside the service’s own domain logic, keep the service’s internal model separate from external DTOs and transport models, don’t let UI or other services dictate internal domain structure, avoid premature splitting into too many microservices, start from a well-structured modular monolith and extract services when boundaries are clear, identify true hotspots for independent scaling or independent change, extract those into services first, watch for distributed monolith anti-patterns where everything still changes together, measure communication patterns and refactor boundaries when necessary, design rollout strategies that respect backward compatibility, use feature flags for behavior changes within services, coordinate long-lived migrations carefully across services and consumers, consider data migration strategies and dual-write or dual-read phases, expose domain events as a primary integration mechanism where appropriate, design event schemas carefully to avoid tight consumer coupling, think through event ordering, idempotency, and replay, avoid overusing event-driven design where a simple synchronous API is enough, keep each service’s codebase small and clean with a clear folder layout, enforce SOLID and clean code principles inside each microservice, write tests close to the behavior, not just lines of code, maintain service-level runbooks and operational docs, standardize dashboards and alerts for critical signals, make it easy to see the health of the whole system at a glance, practice chaos and failure injection in non-production to validate resilience, design security, observability, and operations as cross-cutting pillars, ensure onboarding a new engineer to one service is fast and low-friction, regularly revisit the service landscape and merge or split services when needed, prefer evolving the architecture incrementally over big-bang redesigns, constantly balance service granularity, team autonomy, and operational overhead, and always keep microservices as a tool to serve the domain and the teams, not as a goal in itself






##### FOR MCP SERVERS


начинаем с определения бизнес-способностей и данных, которые вообще имеет смысл открывать через MCP, проектируем каждый MCP-сервер вокруг одного чёткого bounded context и одной основной внешней системы, а не вокруг набора случайных тулов, формулируем доменную модель того, что сервер даёт модели (ресурсы, инструменты, промпты), а уже потом выбираем SDK и транспорт, отделяем доменную логику сервера от протокола MCP и конкретных API внешней системы, инкапсулируем детали внешнего API в адаптерах, не даём им протечь в описания tools/resources, проектируем схемы аргументов и результатов tools как стабильный публичный контракт, версионируем breaking-изменения вместо тихого изменения структуры, делаем каждую операцию идемпотентной насколько возможно (особенно для долгих вызовов и ретраев), заранее закладываем таймауты, отмену и ограничение длительности выполнения тулов, учитываем, что MCP-хост может запускать много параллельных вызовов, проектируем сервер по сути статeless: состояние сессии минимальное, всё важное состояние хранится во внешних системах, явно разделяем слой протокола (JSON-RPC/stdio/HTTP) и слой доменных операций, выстраиваем порты и адаптеры между MCP-контрактом и инфраструктурой, даём каждому модулю сервера одну чёткую ответственность, избегаем god-service который знает и делает всё, программируем против абстракций (интерфейсы репозиториев, клиентов API, провайдеров секретов) а не против конкретных SDK, внедряем зависимости через конструктор/фабрику, не используем скрытые синглтоны и сервис-локаторы, явно прокидываем контекст запроса (user, workspace, permissions, correlation id) через все слои, проектируем авторизацию и ограничения доступа на уровне MCP-сервера (какие ресурсы, какие методы, какие параметры разрешены), реализуем принцип наименьших привилегий к внешним системам (отдельные ключи/аккаунты с минимальными правами), рассматриваем prompt injection и data-exfiltration как ключевые угрозы и фильтруем/ограничиваем то, что сервер отдаёт модели, не даём модели произвольно читать всё подряд, а только явно разрешённые ресурсы, шифруем секреты и конфигурацию, полностью выносим их из кода, используем единый формат конфигурации для всех MCP-серверов (env + config-файлы), логируем все вызовы tools/resources с correlation id, но не логируем чувствительные данные целиком, добавляем метрики по латентности, ошибкам и частоте вызовов на уровень MCP-операций, делаем health-checks и простые диагностические методы для наблюдаемости, проектируем структуру каталогов и модулей сервера так, чтобы она отражала слои: протокол (transport), приложение/use-cases, домен, инфраструктура, чётко разделяем DTO для MCP-контракта и внутренние доменные модели, не тащим наружу внутренние типы, проектируем server-level API минимальным и целевым: лучше несколько маленьких точных tools чем один «universalTool», избегаем chatty-стиля с кучей мелких вызовов, предпочитаем осмысленные coarse-grained операции, проектируем обработку ошибок как часть публичного контракта (свои коды/типы ошибок, понятные сообщения для модели), различаем бизнес-ошибки и технические сбои, для кросс-MCP сценариев и композиций держим контракты максимально простыми и стабильными, с самого начала думаем о многосервисной картине: несколько MCP-серверов под разные домены вместо одного монстра, стандартизируем кросс-серверные вещи (логирование, метрики, трассировка, формат ошибок, правила версионирования), упаковываем каждый сервер как автономный сервис (container, своя конфигурация, свои лимиты ресурсов), делаем запуск/остановку/обновление сервера полностью автоматизируемыми, интегрируем тесты в CI для каждого MCP-сервера отдельно, покрываем доменную логику и маппинг MCP-DTO↔доменные модели юнит-тестами, а работу с реальными внешними системами — интеграционными тестами, обеспечиваем возможность эволюции: новые tools и ресурсы добавляются без ломки старых, старые версии помечаем как deprecated и удаляем по плану, держим документацию по каждому MCP-серверу рядом с кодом (описание tools/resources, схемы, ограничения, примеры), ограничиваем стек технологий для всех MCP-серверов, чтобы упростить сопровождение, проектируем архитектуру MCP-серверов так, чтобы новому разработчику было легко зайти: простые слои, явные границы, минимальная магия, и на всех уровнях поддерживаем строгую разделённость домена, протокола MCP и инфраструктуры, явные контракты и слабую связность между модулями и серверами


start from defining the business capabilities and data it actually makes sense to expose via MCP, design each MCP server around a single clear bounded context and one primary external system rather than a random bag of tools, define the domain model of what the server provides to the model (resources, tools, prompts) before picking SDKs and transports, separate the server’s domain logic from the MCP protocol and from concrete external APIs, encapsulate external API details inside adapters and keep them out of tool/resource schemas, design argument and result schemas for tools as a stable public contract and version them when you need breaking changes, make operations as idempotent as possible to survive retries and long-running calls, build in timeouts, cancellation and execution limits for every tool, assume the MCP host may call many tools in parallel and design for concurrency, keep the server essentially stateless and store important state in external systems, explicitly separate the protocol layer (JSON-RPC / stdio / HTTP) from the application/use-case layer, build ports-and-adapters boundaries between MCP contracts and infrastructure, give each module in the server a single focused responsibility and avoid god-objects that know everything, code against abstractions (interfaces for repositories, API clients, secret providers) instead of concrete SDKs, inject dependencies via constructors/factories instead of hidden singletons or service locators, explicitly propagate request context (user, workspace, permissions, correlation id) through all layers, enforce authorization and access limits at the MCP-server boundary (which resources, which methods, which parameters are allowed), apply least privilege for external systems with dedicated keys/accounts and minimal permissions, treat prompt injection and data exfiltration as first-class threats and strictly control what data the server can return to the model, expose only explicitly allowed resources instead of broad arbitrary access, keep all secrets and configuration out of code and store them encrypted, use a unified configuration approach for all MCP servers (env + config files + secret store), log all tool/resource invocations with correlation IDs while avoiding full dumps of sensitive payloads, add metrics for latency, errors and call frequency per MCP operation, implement health checks and lightweight diagnostic endpoints for observability, structure the project so directories and modules reflect layers (transport/protocol, application/use-cases, domain, infrastructure), separate DTOs for MCP contracts from internal domain models and avoid leaking internal types through public schemas, design the server-level API to be minimal and purposeful, prefer several small precise tools to one giant “do-everything” tool, avoid overly chatty sequences of tiny calls and favor meaningful coarse-grained operations, treat error handling as part of the public contract with clear error types/codes and messages that are useful to the model, distinguish domain/business errors from technical failures, keep cross-MCP scenarios and compositions simple by keeping each server’s contract small and stable, plan from the start for multiple MCP servers instead of a single monster server, group servers by domain and external system to keep them cohesive, standardize cross-server concerns (logging format, metrics, tracing, error shape, versioning rules), package each server as an autonomous service (e.g. container) with its own configuration and resource limits, make starting, stopping and upgrading a server fully automatable, give each MCP server its own CI pipeline with tests and checks, cover domain logic and MCP-DTO↔domain mapping with unit tests, cover real integrations with external systems via integration tests, evolve servers safely by adding new tools/resources without breaking existing ones, deprecate old contracts explicitly and remove them according to a plan, keep documentation per MCP server close to the code (what tools/resources exist, schemas, limits, examples), limit the technology stack across all MCP servers to reduce cognitive load and operational complexity, design the architecture of MCP servers so new developers can onboard quickly (simple layers, explicit boundaries, minimal magic), and at every level maintain strict separation between domain, MCP protocol and infrastructure, explicit contracts, weak coupling between modules and servers, and clear, testable responsibilities.



### telegram bots

начинаем не с команд а с домена бота и пользы для пользователя, формулируем ключевые сценарии: какие роли у бота, какие потоки диалога, какие данные он читает и меняет, описываем доменную модель (пользователь, чат, сессия, задачи/сущности предметной области), разделяем слои: домен, application/use-cases, инфраструктура, интеграции, выделяем Telegram как один из адаптеров на краю системы, не тащим типы и структуры Telegram в домен, маппим Update/Message/CallbackQuery в свои DTO и команды, строим архитектуру вокруг use-case хендлеров а не вокруг Telegram команд, даём каждому хендлеру одну чёткую ответственность, избегаем «universalHandler» который делает всё, отделяем парсинг апдейта от принятия бизнес-решения, держим доменную логику независимой от Telegram SDK и конкретных HTTP-клиентов, программируем против абстракций интерфейсов для отправки сообщений, хранения состояния и доступа к данным, внедряем зависимости в слой use-cases, избегаем глобальных ботов, синглтонов и скрытых сервис-локаторов, явно прокидываем контекст запроса (user id, chat id, язык, права, correlation id) во все слои, проектируем сценарии диалога как конечные автоматы или шаги с явной моделью состояния, не храним сложное состояние в памяти процесса если бот масштабируется горизонтально, выделяем хранение состояния в отдельное хранилище (БД, cache, key-value) с чёткими контрактами, разделяем краткосрочный диалоговый контекст и долгосрочные данные домена, избегаем «магии» с огромным switch по командам, используем маршрутизацию по intent/типу команды и текущему состоянию, отдельно описываем формат команд, кнопок и callback data как стабильный контракт, версионируем callback data и payload’ы если меняем структуру, инкапсулируем форматирование сообщений и клавиатур в отдельный слой presenter/formatter, не смешиваем бизнес-логику с HTML/Markdown разметкой прямо в use-cases, делаем шаблоны сообщений и текстов переиспользуемыми и локализуемыми, сразу закладываем поддержку нескольких языков и часовых поясов, выносим все токены, URL и ключи в конфигурацию и секрет-хранилища, никогда не коммитим токен бота в репозиторий, поддерживаем отдельные конфиги на окружения (dev/stage/prod), явно выбираем способ доставки апдейтов (webhook или long polling) как инфраструктурную деталь, оборачиваем работу с Telegram API в адаптер с ретраями, таймаутами и логированием, учитываем лимиты и rate limit Telegram, проектируем throttling и очереди для массовых рассылок, делаем операции по возможности идемпотентными, учитываем, что один и тот же update может прийти дважды, обрабатываем ошибки Telegram как часть публичного контракта адаптера, различаем бизнес-ошибки (нельзя выполнить команду) и технические (падает API, нет сети), не даём падать всему процессу на одном неудачном апдейте, оборачиваем обработку каждого update в защитную оболочку с try/catch и логированием, логируем ключевые события: входящие апдейты, критичные команды, ошибки, интеграции, добавляем метрики по количеству апдейтов, ошибкам, латентности, строим health-check и простые диагностические эндпоинты, проектируем структуру каталогов отражая архитектуру: domain, application, infrastructure, telegram-адаптер, конфиг, не смешиваем файлы по принципу «по типу фреймворка», держим тесты рядом с доменном и use-cases, покрываем доменную логику юнит-тестами без Telegram, Telegram-адаптер тестируем интеграционно и через контрактные тесты, используем тестовые двойники для внешних API, продумываем безопасность: валидация входных данных, защита от инъекций в callback data, ограничение опасных операций ролями/правами, не шлём в лог чувствительные данные, разделяем приватные чаты и групповые сценарии, явно обрабатываем различия между типами чатов и их правами, проектируем бота так, чтобы можно было добавить новых команд и сценариев без переписывания ядра, выделяем расширяемые точки — регистрацию новых хендлеров, стейт-машин, шаблонов сообщений, делаем публичное API модулей минимальным и стабильным, избегаем god-сервисов типа BotService который знает обо всём на свете, делаем бот как тонкий слой над хорошо спроектированным приложением/доменом, чтобы его же логику можно было использовать из других интерфейсов (web, CLI, MCP, microservice), автоматизируем сборку, деплой и миграции, поддерживаем возможность отката версии бота, проектируем систему так, чтобы новый разработчик по структуре каталогов и контрактам быстро понял, что где, и во всём стеке бота поддерживаем строгие границы слоёв, явные контракты, слабую связность и тестируемость.


start from the bot’s domain and the value it brings to users, define the key scenarios and conversation flows before thinking about commands, model the core domain concepts (user, chat, session, domain entities) explicitly, structure the system into domain, application/use-case, infrastructure, and telegram-adapter layers, treat Telegram as just one adapter at the edge of the system, avoid pulling Telegram types and DTOs into the domain model, map Update / Message / CallbackQuery to your own commands and DTOs, design architecture around use-case handlers instead of raw Telegram commands, give each handler a single clear responsibility, avoid a single giant “universal handler” that does everything, separate update parsing from business decision-making, keep business logic independent of the Telegram SDK and HTTP client details, code against abstractions for message sending, state storage, and data access, inject these dependencies into use-case layers instead of using globals, avoid global bot instances, singletons, and service locators, explicitly pass request context (user id, chat id, locale, permissions, correlation id) through all layers, model conversational flows and wizards as finite state machines or explicit step models, don’t store complex conversational state only in process memory if you plan horizontal scaling, keep state in a dedicated storage (DB, cache, key-value) with clear contracts, separate short-lived conversational context from long-lived domain data, avoid huge switch/if chains on commands scattered across the codebase, centralize routing by intent/command type and current state, define the format of commands, buttons, and callback data as a stable contract, version callback payloads if you change their structure, encapsulate message formatting and keyboards in a presenter/formatter layer, do not mix business logic with HTML/Markdown layout in use-cases, make message templates reusable and localizable, plan for multiple languages and time zones from the start, move all tokens, URLs, and secrets out of code into configuration and secret stores, never commit the bot token to the repository, maintain separate configs per environment (dev/stage/prod), treat choice of delivery mechanism (webhook vs long polling) as an infrastructure concern, wrap Telegram API access in an adapter with retries, timeouts, and logging, respect Telegram rate limits and design throttling and queues for bulk sends, make operations as idempotent as possible and handle duplicate updates safely, treat Telegram API errors as part of the adapter’s public contract, distinguish between business errors (command cannot be executed) and technical errors (network/API failure), prevent a single bad update from crashing the whole process, wrap each update handling in a safe boundary with proper error handling and logging, log key events such as incoming updates, critical commands, and integration calls, add metrics for update throughput, error rates, and handler latency, expose health checks and lightweight diagnostic endpoints for operations, design the folder and module structure to reflect the architecture (domain, application, infrastructure, telegram) rather than framework accidents, keep tests close to domain and use-cases, cover domain logic with unit tests that don’t depend on Telegram, test the Telegram adapter via integration and contract tests, use test doubles for external APIs and storages, design for security from the start with strict validation of incoming data, protect against injection and malformed callback data, enforce roles/permissions for sensitive operations, avoid logging sensitive information, separate private chat flows from group chat flows explicitly, handle the differences between chat types and their capabilities, design the bot so new commands and flows can be added by registering new handlers and states, not by rewriting the core, define clear extension points for new handlers, state machines, and message templates, keep public module APIs minimal and stable, avoid “BotService” god-objects that know everything, make the bot a thin interface over a well-designed application/domain layer so the same logic could be reused from web, CLI, MCP, or microservices, automate build, deployment, and migrations, support safe rollback of bot versions, design the system so that a new developer can understand the structure and contracts quickly, and throughout the bot keep strict separation of concerns between layers, explicit contracts, weak coupling, and high testability.



#### Telegram Aiogram

начинаем не с aiogram и команд, а с домена бота и пользы для пользователя, формулируем ключевые сценарии и флоу диалогов, выделяем основные сущности домена (пользователь, чат, сессия, предметные объекты), строим архитектуру по слоям: domain, application/use-cases, infrastructure, telegram/aiogram-адаптер, рассматриваем Telegram и aiogram как инфраструктурный слой на границе системы, не тянем типы Message, CallbackQuery, Update и прочие объекты aiogram в доменную модель, маппим апдейты в свои команды/DTO на границе слоя, строим систему вокруг use-case / application-хендлеров, а не вокруг сырых Telegram-команд, даём каждому use-case одну чёткую ответственность, избегаем одного огромного хендлера «на всё», отделяем разбор апдейта и роутинг (filters, routers) от бизнес-логики, держим логику сценариев независимой от aiogram и деталей Telegram API, используем абстракции для отправки сообщений, работы с хранилищем и внешними сервисами, не привязываем домен к конкретным clients/ORM/SDK, внедряем зависимости в use-case слои через конструкторы/фабрики, не используем скрытые синглтоны, глобальные Bot/Dispatcher как «магические» глобальные точки доступа, инициализируем бот, диспетчер, хранилища и клиенты во входной точке приложения (composition root) и дальше явно прокидываем их в нужные модули, проектируем маршрутизацию через Router’ы по доменным модулям (отдельные роутеры на отдельные подсистемы, разделяем команды, callback-и, inline и т.п.), избегаем одного большого router с сотнями хендлеров, используем фильтры aiogram для декларативной маршрутизации, но держим фильтры как тонкий слой над доменными условиями, выносим общие кросс-срезы (логирование, аутентификация, загрузка пользователя, локаль, трейсинг, rate limiting) в middlewares, не размазываем их по всем хендлерам руками, моделируем сложные диалоги через FSM/машину состояний aiogram, но состояние описываем доменными терминами, а не «State1/State2», разделяем краткоживущее диалоговое состояние (шаг формы, выбор варианта) и долгоживущие сущности домена (заказы, задачи, профили и т.п.), не рассчитываем только на in-memory FSM, если бот планируется к горизонтальному масштабированию — используем внешнее хранилище состояний (Redis, БД) через адаптер, чётко определяем, какие данные хранятся в FSM, какие — в доменной БД, описываем формат команд, callback data и payload-ов как стабильный контракт, используем структурированный callback data (например, action:entity:id) и версионируем его при изменениях, не шифруем смысл в непонятные строки-UUID без причины, выносим форматирование сообщений, клавиатур и inline-кнопок в отдельный слой presenters/formatters, не смешиваем бизнес-логику с HTML/Markdown прямо в хендлерах, храним тексты и шаблоны отдельно, закладываем локализацию: используем i18n (aiogram-i18n/любой другой подход), не хардкодим тексты в коде, учитываем таймзоны и локали пользователя при форматировании дат/времени, выносим токен бота, URL-ы, ключи, флаги и прочую конфигурацию в env и конфиг-файлы, ни при каких условиях не коммитим токен бота в репозиторий, делаем отдельные конфигурации для dev/stage/prod, учитываем, что выбор между webhook и long polling — инфраструктурное решение: оборачиваем запуск/конфигурацию вебхука в отдельный слой, не смешиваем это с доменом, оборачиваем Telegram API в свой сервис/адаптер поверх Bot, добавляем туда ретраи, таймауты, логирование и обработку rate limits, дизайн операций делаем максимально идемпотентным, учитываем, что один update_id может прилететь повторно, явно обрабатываем ошибки Telegram и коды ответов, различаем бизнес-ошибки (пользователь сделал недопустимое действие по доменным правилам) и технические (упало API, таймаут, сеть), не даём упасть всему приложению при исключении в одном хендлере, оборачиваем обработку каждого апдейта try/except-оболочкой и логированием, используем middlewares для логирования и трейсинга входящих апдейтов и исходящих запросов к Telegram и внешним системам, логируем ключевые события: новые пользователи, важные команды, неудачные попытки выполнения доменных операций, добавляем метрики: количество апдейтов, ошибок по типам, латентность хендлеров, частоту вызова отдельных use-cases, делаем health-check эндпоинты для проверки зависимостей (БД, внешние API) и самого процесса, проектируем структуру проекта так, чтобы она отражала архитектуру: отдельные пакеты/модули для domain, usecases (application), infra (БД, внешние сервисы, репозитории), telegram (routers, handlers, middlewares, formatters), не строим структуру только вокруг handlers.py и keyboards.py, держим хендлеры максимально тонкими: парсим вход, передаём команду в use-case, получаем результат, форматируем ответ, покрываем доменную логику и use-cases модульными тестами без aiogram, Telegram-адаптер и интеграции с внешними API проверяем интеграционными/контрактными тестами, используем fixtures и test storages, продумываем безопасность: валидируем входящие данные (особенно callback data, user input, inline queries), не доверяем содержимому апдейта, ограничиваем опасные операции по ролям/правам, отделяем сценарии в приватных чатах от сценариев в группах/супергруппах, учитываем различия в поведении и правах, не логируем и не показываем стеки/детали ошибок пользователю, но даём пользователю понятные сообщения об ошибке/недоступности функционала, проектируем бота расширяемым: новые команды и флоу добавляются через новые routers/handlers/use-cases, а не через переписывание существующих монолитных файлов, выделяем явные точки расширения (регистрация роутеров, новое состояние, новый formatter), избегаем god-объектов вроде одного огромного BotService/Core на всё, делаем бота тонким интерфейсом над хорошо спроектированным приложением/доменом, чтобы при необходимости ту же логику можно было вызвать из других интерфейсов (web, CLI, микро-сервис, MCP), автоматизируем сборку, деплой и миграции (Docker, CI/CD, alembic/аналог для БД), закладываем возможность безопасного отката версии, фиксируем архитектурные решения в коротких ADR, чтобы команда понимала «почему так», проектируем всё так, чтобы новому разработчику было легко зайти: читаемая структура каталогов, понятные имена, явные контракты между слоями, минимум «чёрной магии», и во всём коде бота на aiogram поддерживаем строгие границы слоёв, явные контракты, слабую связность, тестируемость и предсказуемое поведение.



#### WPF C#


начинаем не с XAML и контролов, а с домена приложения и пользовательских сценариев, формулируем, какие задачи решает десктоп-приложение и какие сущности домена в нём есть, строим архитектуру по слоям: domain (модели и бизнес-правила), application/services (use-cases, оркестрация), infrastructure (БД, файлы, сети), presentation (WPF UI, View + ViewModel), используем MVVM как основной паттерн, чётко разделяем View, ViewModel и Model, избегаем бизнес-логики в code-behind и в XAML-триггерах, держим code-behind максимально тонким (инициализация, мелкие UI-хаки, которые нельзя выразить иначе), всё поведение и состояние экрана выносим во ViewModel, программируем против абстракций: ViewModel не знает о конкретных контролах и окнах, ViewModel общается с внешним миром через сервисы и интерфейсы, используем INotifyPropertyChanged и биндинги как основной механизм связи UI и ViewModel, избегаем ручного FindName и прямого обращения к контролам из логики, строим взаимодействие через привязки свойств, ICommand и события/мессенджеры, определяем ViewModel’ы вокруг экранов и пользовательских сценариев, а не вокруг таблиц БД и форм, даём каждому ViewModel одну чёткую ответственность, избегаем гигантских «MainViewModel» и «GodViewModel», декомпозируем сложные экраны на композицию мелких View + ViewModel, выделяем общие компоненты и переиспользуем их, навигацию (открытие окон, страниц, диалогов) инкапсулируем в отдельный сервис навигации, ViewModel не создаёт напрямую окна и не вызывает Show()/ShowDialog(), навигация строится через команды и абстракции, внедряем зависимости в ViewModel через конструктор и DI-контейнер, а не создаём сервисы внутри через new, не используем сервис-локаторы и глобальные синглтоны, конфигурацию, подключения и пути выносим в настройки и конфиг-файлы, не хардкодим их в коде, держим доменную логику чистой и независимой от WPF и UI-фреймворка, любые обращения к БД, файловой системе, сети, диалогам, печати выносим в адаптеры и инфраструктурные сервисы, ViewModel обращается к ним через интерфейсы, разделяем модели домена и модели представления (DTO/ViewModel), не тянем напрямую сущности ORM в биндинги, проектируем валидацию данных на уровне ViewModel и домена (IDataErrorInfo/INotifyDataErrorInfo), не кладём всю валидацию только в UI, используем команды (ICommand, Relay/DelegateCommand) вместо обработки кликов в code-behind, вся реакция на действие пользователя живёт во ViewModel, продумываем асинхронность: долгие операции выполняем через async/await и фоновые задачи, никогда не блокируем UI-поток Thread.Sleep или Task.Result, показываем индикаторы загрузки и не даём пользователю «подвешивать» приложение, используем ObservableCollection и другие коллекции, поддерживающие уведомления, для списков в UI, не обновляем UI-коллекции вручную без уведомлений, инкапсулируем доступ к БД и файлам в репозитории/сервисы, не пишем SQL/IO прямо в ViewModel, отделяем шаблоны стилей, ресурсов и визуальных элементов в ResourceDictionary и отдельные XAML-файлы, не превращаем App.xaml и главное окно в свалку ресурсов, централизуем стили и тему приложения, закладываем локализацию: строки интерфейса выносим в ресурсы, не хардкодим текст в XAML и коде, следим за тем, чтобы ViewModel не зависел от конкретного DI-контейнера, логгера, ORM — используем интерфейсы и адаптеры, добавляем логирование ключевых действий и ошибок в инфраструктурном слое, не показываем пользователю «сырае» исключения, а даём дружелюбные сообщения, обрабатываем ошибки асинхронных операций и не глушим их в пустых catch, проектируем структуру проектов так, чтобы она отражала архитектуру: отдельные сборки/проекты для домена, приложений/сервисов, инфраструктуры и WPF-UI, не смешиваем всё в одном гигантском проекте, пишем юнит-тесты для доменных моделей и сервисов, ViewModel тестируем отдельно от реального WPF, подменяя сервисы тестовыми двойниками, минимизируем логику, завязанную именно на UI-поток, чтобы упростить тестирование, продумываем старт приложения: в App/bootstrapper создаём контейнер, регистрируем сервисы, ViewModel и View, настраиваем маппинг View ↔ ViewModel, не размазываем инициализацию по всем углам, для сложных приложений используем чёткий подход к модульности (модули/области, отдельные части UI и домена), избегаем плотной связности модулей, общение между модулями строим через события, сообщения или сервисы, а не через прямые ссылки, следим за производительностью: тяжёлые визуальные эффекты, сложные шаблоны и триггеры используем осознанно, профилируем, не оптимизируем «на глаз», используем VirtualizingStackPanel и виртуализацию для длинных списков, управляем ресурсами (подписки на события, таймеры, потоки), не допускаем утечек памяти из-за незавершённых подписок и сильных ссылок, явно освобождаем ресурсы, когда окна и ViewModel больше не нужны, документируем ключевые архитектурные решения (паттерн MVVM, подход к навигации, DI, структуру слоёв) хотя бы в виде кратких ADR, придерживаемся одного стиля кодирования и XAML-разметки в команде, автоматизируем сборку и проверку (анализаторы, StyleCop, Roslyn-анализаторы), проектируем WPF-приложение так, чтобы новому разработчику было понятно, где домен, где инфраструктура, где View и ViewModel, и во всём коде строго поддерживаем разделение ответственности, явные границы между слоями, слабую связность и тестируемость.
